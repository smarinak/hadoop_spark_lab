1. Устанавливаем и настраиваем Docker Desktop
2. Создаем папку проекта (например `~/hadoop_spark_docker`)
3. В этой папке создаем файл `docker-compose.yml` (файл с необходимым содержимым представлен в репозитории)
4. Запускаем кластер и проверяем, что все работает

    Переходим в папку проекта:
    
    ```bash
    cd ~/hadoop_spark_docker
    ```
    
    Поднимаем сервисы:
    
    ```bash
    docker compose up -d
    ```
    
    Проверяем, что все работает:
    
    ```bash
    docker ps
    ```
    
    *должно отобразиться 4 строчки с запущенными контейнерами
    
5. Загружаем данные в HDFS
    
    Копируем файл с хоста в контейнер `namenode`
    
    ```bash
    docker cp /ваш/путь/до/файла/tweets.txt namenode:/tmp/tweets.txt
    ```
    
    Заходим внутрь контейнера `namenode`
    
    ```bash
    docker exec -it namenode bash
    ```
    
    *видим командную строку namenode
    
    Создаём директорию для пользователя Spark
    
    ```bash
    hdfs dfs -mkdir -p /user/spark
    ```
    
    Проверяем, что директория создалась
    
    ```bash
    hdfs dfs -ls /user
    ```
    
    Переносим файл в hdfs
    
    ```bash
    hdfs dfs -put -f /tmp/tweets.txt /user/spark/tweets.txt
    ```
    
6. Задания
   
    **Как писать код и запускать его?**
   
    Код пишем у себя на компьютере, сохраняем в любой папке.
   
    Например, в `~/hadoop_spark_docker` создаем папку `apps` и в ней сохраняем файл с расширением .py для каждой задачи.
   
    **Чтобы запустить файл с кодом, вводим в командной строке следующее:**
    
    ```bash
    docker run --rm -it \
      --network hadoop_spark_docker_default \
      -v /путь/до/папки/hadoop_spark_docker/apps:/tmp \
      --platform linux/amd64 \
      bde2020/spark-submit:3.1.1-hadoop3.2 \
      /spark/bin/spark-submit \
      --master spark://spark-master:7077 \
      /tmp/ваш_файл.py
    ```
    
    **Как посмотреть результат?**
   
    Все зависит от вашего кода, можно выводить данные в командной строке или сохранять в файл в hdfs.
    Желательно использовать второй вариант.
    
    ```python
    counts.saveAsTextFile("hdfs://namenode:9000/user/spark/output_wordcount_tweets")
    # counts — это объект RDD в PySpark, который хранит результат подсчёта слов
    ```
    
    **Как считать содержимое файла, сохраненное в hdfs?**
    
    Заходим внутрь контейнера `namenode`
    
    ```bash
    docker exec -it namenode bash
    ```
    
    Выводим файлы с результатом
    
    ```bash
    hdfs dfs -ls /user/spark/output_wordcount_tweets
    ```
    
    Выводим содержимое файла
    
    ```bash
    hdfs dfs -cat /user/spark/output_wordcount_tweets/part-00000
    ```
    
    **Задача 1.** 
    
    Написать код для подсчета слов в файле tweets.txt с использованием pyspark. Вывести топ 50 самых часто используемых слов.
    
    **Задача 2.** 
    
    Загрузить `online_retail.csv` в hdfs и провести очистку данных:
    
    - удалите `null` из столбца `CustomerID`
    - удалите строки, где `Quantity` и `Price` отрицательные
    - удалить строки, `Invoice` начинается с буквы “C” (пометка возвратов)
    
    **Задача 3.**
    
    Проверить наличие выбросов. Удалить, если они есть.
    
    - Для каждого числового столбца (`Quantity`, `Price`) посчитать 1-й (Q1) и 3-й (Q3) квартиль.
    - Вычислить IQR = Q3 - Q1.
    - Определить границы выбросов:
        - Нижняя граница = Q1 - 1.5 * IQR
        - Верхняя граница = Q3 + 1.5 * IQR
    - Найти строки, где значения выходят за эти границы.
    - Посчитать количество выбросов по каждому столбцу.
    
    **Задача 4.**
    
    - Найти топ-3 страны по количеству покупок.
    - Посчитать количество транзакций по месяцам.
    - Вычислить средний чек (сумма покупки на одну транзакцию) по клиентам.
    
    **Задача 5.**
    
    Определить клиентов, которые приносят наибольший доход и чаще покупают.
    Посчитать метрики:
    
    - **Recency (давность последней покупки)** – сколько дней прошло с последней покупки клиента.
    - **Frequency (частота покупок)** – сколько покупок совершил клиент за период.
    - **Monetary (сумма покупок)** – сколько денег клиент потратил.
    
    Отсортировать клиентов по всем трем метрикам так, чтобы наверху были те, кто приносит наибольший доход и чаще покупает.
